{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lyricsgenius\n",
    "genius_TOKEN = 'sD0C3epnJdfOQQK4eIC45dHl-Qv7DipToGpuj1n4WeuG5_LDP1HKn31w5Cn1lOux'\n",
    "genius = lyricsgenius.Genius(genius_TOKEN)\n",
    "genius.verbose = False\n",
    "genius.remove_section_headers = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('merged_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(df):\n",
    "    genres = []\n",
    "    languages = []\n",
    "    countries = []\n",
    "    for x,y,z in zip(df['Genres'],df['Languages'],df['Countries']):\n",
    "        g = re.findall(': \\\"(.*?)\\\"', x)\n",
    "        l = re.findall(': \\\"(.*?)\\\"', y)\n",
    "        c = re.findall(': \\\"(.*?)\\\"', z)\n",
    "        genres.append(g)\n",
    "        languages.append(l)\n",
    "        countries.append(c)\n",
    "    df['Genres'] = genres\n",
    "    df['Languages'] = languages\n",
    "    df['Countries'] = countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "listify(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=50000, max_df=0.8, min_df=20, norm='l2')\n",
    "tokenizer = vectorizer.build_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['toks'] = [tokenizer(summary) for summary in movies['Summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_english = ['English Language' in m for m in movies['Languages']]\n",
    "#movies = movies[is_english]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''years = []\n",
    "for x in movies['ReleaseDate']:\n",
    "    x = str(x)[:4]\n",
    "    if x != 'nan':\n",
    "        years.append(int(x))\n",
    "    else:\n",
    "        years.append(3000)\n",
    "movies['ReleaseDate'] = years'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movies = movies[movies['ReleaseDate']>=1960]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movies = movies.reset_index()\n",
    "#movies = movies.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_movies = len(movies)\n",
    "num_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inverted_index(movies):\n",
    "    word_set = []\n",
    "    for t in movies['toks']:\n",
    "        word_set+=t\n",
    "    word_set = set(word_set)\n",
    "    word_dict = {w: [] for w in word_set}\n",
    "    for i in range(len(movies)):\n",
    "        for w in set(movies['toks'][i]):\n",
    "            if w in movies['toks'][i]:\n",
    "                word_dict[w].append((i,movies['toks'][i].count(w)))\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#took long time to run, only needs to be done once\n",
    "#inv_idx = build_inverted_index(movies)\n",
    "#np.save('inv_idx.npy', inv_idx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = np.load('inv_idx.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inv_idx.pkl', 'wb') as f:\n",
    "    pickle.dump(inv_idx, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inv_idx.txt', 'w') as file:\n",
    "    json.dump(inv_idx, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inv_idx.txt', 'r') as file:\n",
    "    new_d = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(inv_idx, n_docs, min_df=20, max_df_ratio=0.8):\n",
    "    idf = {x: math.log2(n_docs/(1+len(inv_idx[x]))) for x in inv_idx \n",
    "           if len(inv_idx[x])>=min_df and len(inv_idx[x])/n_docs<=max_df_ratio}\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = compute_idf(inv_idx,num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_norms(index, idf, n_docs):\n",
    "    norms_sq = np.zeros(n_docs)\n",
    "    for t in idf:\n",
    "        for (doc,cnt) in index[t]:\n",
    "            norms_sq[doc] += (cnt*idf[t])**2\n",
    "    return np.sqrt(norms_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norms = compute_doc_norms(inv_idx, idf, num_movies)\n",
    "#np.savetxt('norms.csv', norms, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "norms = np.loadtxt('norms.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanjson(result):\n",
    "    title = result[result.find(\"Title\")+8:result.find(\"Year\")-3]\n",
    "    plot = result[result.find(\"Plot\")+7:result.find(\"Language\")-3]\n",
    "    try:\n",
    "        review_imdb = float(result[result.find(\n",
    "        '\"Internet Movie Database\",\"Value\":\"')+35:result.find('Source\":\"Rotten Tomatoes\"')-8])\n",
    "    except:\n",
    "        review_imdb = -1\n",
    "    try:\n",
    "        review_rotten = float(result[result.find(\n",
    "        'Source\":\"Rotten Tomatoes\",\"Value\":')+35: result.find('},{\"Source\":\"Metacritic\"')-2])\n",
    "    except:\n",
    "        review_rotten = -1\n",
    "    return [title, plot, review_imdb, review_rotten]\n",
    "\n",
    "def response(result):\n",
    "    text = result[result.find('\"Response\":' )+12:]\n",
    "    return text.find('True') >-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "omdb_TOKEN = 'ce887dbd'\n",
    "def find_movie(movie):\n",
    "    title = \"\"\n",
    "    plot = \"\"\n",
    "    query = \"http://www.omdbapi.com/?apikey=\" + omdb_TOKEN + \"&t=\" + movie\n",
    "    params = {\"r\": \"json\", \"plot\": \"full\"}\n",
    "    result = requests.get(query, params)\n",
    "    if response(result.text):\n",
    "        json = cleanjson(result.text)\n",
    "        plot = json[1]\n",
    "        title = json[0]\n",
    "        review_imdb = json[2]\n",
    "        review_rotten = json[3]\n",
    "    else:\n",
    "        return \"ERROR\"\n",
    "    return (title, plot, review_imdb, review_rotten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_search(user_mov):\n",
    "    movie = find_movie(user_mov)\n",
    "    query = movie[1]\n",
    "    #query = ''\n",
    "    #if movie[0] in movie_titles:\n",
    "    #    query = movies['Summary'][title_to_index[movie[0]]]\n",
    "    #else:\n",
    "    #    query = movie[1]\n",
    "    scores = np.zeros(len(norms))\n",
    "    docs = [i for i in range(len(norms))]\n",
    "    q = query.lower()\n",
    "    q_tokens = tokenizer(q)\n",
    "    q_norm_sq = 0\n",
    "    for t in set(q_tokens):\n",
    "        if t in idf:\n",
    "            q_norm_sq += (q_tokens.count(t)*idf[t])**2\n",
    "            for (doc,cnt) in inv_idx[t]:\n",
    "                scores[doc] += (q_tokens.count(t)*cnt*idf[t]**2)/norms[doc]\n",
    "    q_norm = math.sqrt(q_norm_sq)\n",
    "    new_scores = [score/q_norm for score in scores]\n",
    "    result = sorted(tuple(zip(new_scores, docs)),reverse=True)\n",
    "    return result[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index_search('Memento')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.16056939582947824, 35155),\n",
       " (0.15718873462135707, 13330),\n",
       " (0.15580892217745576, 37607),\n",
       " (0.1485379138540403, 2487),\n",
       " (0.14042652859711635, 34573),\n",
       " (0.13924014073325505, 17758),\n",
       " (0.1351215565899785, 15566),\n",
       " (0.1330696150046359, 3533),\n",
       " (0.13249725484059963, 41117),\n",
       " (0.12548895093423626, 2898)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_10(movie,results):\n",
    "    ten = []\n",
    "    for sim, ind in results[:10]:\n",
    "        if movies['Title'][ind] != movie:\n",
    "            s = movies['Title'][ind]+' (score: '+str(sim)+')'\n",
    "            ten.append(s)\n",
    "    return ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frauds (score: 0.16056939582947824)',\n",
       " 'Duel in the Jungle (score: 0.15718873462135707)',\n",
       " 'The Fire Raisers (score: 0.15580892217745576)',\n",
       " 'Cover Up (score: 0.1485379138540403)',\n",
       " \"Killer's Carnival (score: 0.14042652859711635)\",\n",
       " 'Perdido por perdido (score: 0.13924014073325505)',\n",
       " 'The Gentleman from Nowhere (score: 0.1351215565899785)',\n",
       " 'Spring Fever (score: 0.1330696150046359)',\n",
       " 'Adulterous Wife: Dizzy (score: 0.13249725484059963)',\n",
       " 'Zombie Girl: The Movie (score: 0.12548895093423626)']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_10('Memento',results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c67d541c1077>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmovies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m35155\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "movies['Title'][35155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id_to_index = {wikiid:i for i, wikiid in enumerate(movies['WikiID'])}\n",
    "#title_to_id = {t:i for t, i in zip(movies['Title'], movies['WikiID'])}\n",
    "#id_to_title = {v:k for k,v in title_to_id.items()}\n",
    "#title_to_index = {t:i for i, t in enumerate(movies['Title'])}\n",
    "#index_to_title = {v:k for k,v in title_to_index.items()}\n",
    "#movie_titles = [t for t in movies['Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_by_vocab = vectorizer.fit_transform([s for s in movies['Summary']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(movie, data):\n",
    "    wiki_id = title_to_id.get(movie)\n",
    "    ind = id_to_index.get(wiki_id)\n",
    "    summary = data['Summary'][ind]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(user_mov, other_mov):\n",
    "    j = doc_by_vocab[title_to_index[other_mov]]\n",
    "    result = find_movie(user_mov)\n",
    "    if result[0] in movie_titles:\n",
    "        i = doc_by_vocab[title_to_index[result[0]]]\n",
    "    else:\n",
    "        i = vectorizer.transform([result[1]]).toarray()[0]\n",
    "    cosim = np.dot(i,j) / (np.linalg.norm(i)*np.linalg.norm(j))\n",
    "    return cosim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
